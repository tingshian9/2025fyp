# Sentiment Trend Predictor (Streamlit)

A simple end‚Äëto‚Äëend prototype that **trains a sentiment classifier** on the Sentiment140 dataset and **predicts whether a topic is likely to trend** based on the sentiment ratio and tweet volume for a chosen keyword and date range. The project includes a training script (`model_script.py`) and a Streamlit UI (`app.py`).

---

## ‚ú® Features

- **One‚Äëtime training** on Sentiment140 (via `kagglehub`) with TF‚ÄëIDF + Logistic Regression
- **Auto‚Äëcache & reuse**: loads existing `model.pkl` and `vectorizer.pkl` if present; otherwise trains and saves them
- **Lightweight text cleaning** (link, mention, hashtag removal, alphanumerics only, lowercase, English stopwords)
- **Keyword search + date range filter** over the dataset
- **Trend score** = sentiment ratio √ó log(1 + tweet volume); boolean **`will_trend`** from threshold
- **Model metrics** saved to `data/metrics.json` and shown in the app (Accuracy, Precision, Recall, F1)
- **Streamlit UI** with inputs for keyword, date range, and sentiment target

> NOTE: The UI currently navigates to `pages/results.py`. If that file does not exist in your project yet, see the ‚ÄúResults page note‚Äù below for quick options.

---

## üóÇ Project Structure

```
.
‚îú‚îÄ‚îÄ app.py                 # Streamlit UI
‚îú‚îÄ‚îÄ model_script.py        # Training + prediction helpers
‚îú‚îÄ‚îÄ model.pkl              # Saved logistic regression model (created after training)
‚îú‚îÄ‚îÄ vectorizer.pkl         # Saved TF‚ÄëIDF vectorizer (created after training)
‚îî‚îÄ‚îÄ data/
    ‚îú‚îÄ‚îÄ training.1600000.processed.noemoticon.csv   # Sentiment140 copy (auto‚Äëdownloaded)
    ‚îî‚îÄ‚îÄ metrics.json         # Evaluation metrics (created after training)
```

`model.pkl`, `vectorizer.pkl`, and `data/metrics.json` are generated by `model_script.py`. The dataset is copied to `./data/` on first run.

---

## üß† How it works

### Training pipeline (`model_script.py`)

1. **Download dataset** using `kagglehub` and copy it to `./data/training.1600000.processed.noemoticon.csv` (skips if already present).
2. **Load & prepare columns**: sentiment (0/4 ‚Üí 0/1), timestamp (parsed to datetime), and raw text.
3. **Clean text**: remove URLs, @mentions, hashtags symbols; keep alphanumerics; lowercase; remove English stopwords.
4. **Vectorize** with `TfidfVectorizer(max_features=5000)` and **train** a `LogisticRegression` classifier.
5. **Evaluate** on a hold‚Äëout split; **save metrics** to `data/metrics.json`.
6. **Persist artifacts**: write `model.pkl` and `vectorizer.pkl` for later reuse.

On subsequent runs, if `model.pkl` and `vectorizer.pkl` exist, the script **loads** them instead of retraining.

### Inference helpers

- **`predict_trend(tweet_list, target_sentiment="Positive", sentiment_threshold=0.6)`**  
  Produces probabilities with the trained model, computes a **sentiment ratio** (share of tweets ‚â• threshold for ‚ÄúPositive‚Äù, or ‚â§ 1‚àíthreshold for ‚ÄúNegative‚Äù), and a **trend score**:

  	**trend_score = sentiment_ratio √ó log(1 + tweet_volume)**

  Returns: `('trend_score', 'sentiment_ratio', 'tweet_volume')` (and `will_trend` in keyword mode).

- **`run_keyword_prediction(keyword, df, sentiment_target, start_date, end_date, model, vectorizer)`**  
  Filters the dataset by **keyword** and **date range**, then computes the same metrics. If fewer than 30 tweets match, it returns a ‚Äúnot enough tweets‚Äù message.

---

## üñ• Streamlit app (`app.py`)

- **Inputs**:  
  - Text input for **keyword**  
  - **Date range** selector (bounded by the dataset‚Äôs min/max timestamps)  
  - **Sentiment filter**: *Positive* or *Negative*

- **Predict button**: saves selections in `st.session_state` and navigates to **`pages/results.py`**.

- **Metrics expander**: reads `data/metrics.json` and displays **Accuracy, Precision, Recall, F1‚ÄëScore** with `st.metric`.

### Results page note

The UI calls `st.switch_page("pages/results.py")`. If you don‚Äôt yet have that page, you have two options:

1. **Create `pages/results.py`** and read values from `st.session_state` to run `run_keyword_prediction(...)`, then display the outputs with charts/tables and (optionally) an ‚ÄúExport CSV‚Äù button.
2. **Inline prediction** (quick fallback): uncomment the existing block in `app.py` that calls `run_keyword_prediction(...)` directly after clicking **Predict** (the block is already present but commented out).

---

## üöÄ Setup & Run

### 1) Environment

- Python 3.10+ is recommended.

### 2) Install dependencies

```bash
pip install streamlit pandas numpy scikit-learn nltk kagglehub
```

> The app/script will download NLTK English stopwords at runtime if needed.

### 3) Train (or load) the model

```bash
python model_script.py
```

This will download the dataset (first run), train/evaluate (or load existing artifacts), save metrics, and write `model.pkl` + `vectorizer.pkl`.

### 4) Launch the UI

```bash
streamlit run app.py
```

Open the provided local URL in your browser, enter a **keyword**, choose a **date range**, pick a **sentiment target**, and click **Predict**.

---

## ‚öôÔ∏è Configuration & Paths

- **Artifacts**: `model.pkl`, `vectorizer.pkl` in project root.  
- **Dataset/metrics**: under `./data/`.  
- **Date bounds** in the UI are inferred from the dataset‚Äôs timestamp column.  
- **Thresholds**: by default, 0.6 for positive and 0.4 for negative when computing the sentiment ratio in keyword mode.

---

## üß™ Typical Workflow

1. Run `python model_script.py` to (re)train and generate metrics.  
2. Start the app with `streamlit run app.py`.  
3. Try keywords like `iPhone`, `Amazon`, `Netflix`, etc.  
4. Toggle sentiment target and adjust date range to explore how **sentiment ratio** and **tweet volume** affect the **trend score**.

---

## üõ† Troubleshooting

- **`‚ö†Ô∏è Model metrics not available` in the app**  
  Run `python model_script.py` to regenerate `data/metrics.json`.

- **Module import issues** (e.g., cannot import `run_keyword_prediction`)  
  Ensure `app.py` and `model_script.py` are in the **same directory**, and you‚Äôre launching Streamlit from that directory.

- **Kaggle download errors**  
  Check your internet connection and make sure `kagglehub` is installed. As a fallback, manually place `training.1600000.processed.noemoticon.csv` in `./data/`.

- **Results page missing**  
  Create `pages/results.py` or switch to the inline prediction approach described above.

---

## üó∫ Roadmap ideas (optional)

- Add a **Results page** with charts (e.g., distribution of predicted probabilities, time series by day).  
- Add an **Export CSV** of matched tweets with predicted probabilities.  
- Parameterize thresholds and trend cutoff.  
- Add caching for faster repeated keyword/date queries.

---

## üìÑ License

Choose a license (e.g., MIT) and add it here.
# 2025fyp
